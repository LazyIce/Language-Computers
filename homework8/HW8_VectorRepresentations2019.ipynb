{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector representations of words and documents\n",
    "\n",
    "This homework is significantly adapted by Lelia Glass from one developed at UPenn by Daphne Ippolito, Anne Cocos, Stephen Mayhew, and Chris Callison-Burch.  \n",
    "\n",
    "An important idea throughout this assignment (and in computer science in general these days) is VECTORIZATION: the idea of performing operations on entire matrices all at once, rather than writing \"for loops\" for each individual element.  Even when you understand how to calculate each individual element on its own, \"vectorizing\" the computation can be a bit tricky, so don't worry if you don't understand it all yet.  This is a learning process!\n",
    "\n",
    "I also want to point out that even if you don't know much programming, you should be able to follow along in this assignment, which asks you to READ and UNDERSTAND code more than writing it yourself.  Just try to understand as much as you can, and you'll move forward in your learning process, even if you do not understand it all yet.  That's okay!\n",
    "\n",
    "The assignment is based on Chapter 6 of Jurafsky and Martin (3rd Edition).  Please read that chapter to fully understand what's happening here.  You can also check out these slides: https://web.stanford.edu/~jurafsky/li15/lec3.vector.pdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import subprocess\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out how this function works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term-by-document matrix\n",
    "\n",
    "First, we write code to create a term-by-document matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_term_document_matrix(line_tuples, document_names, vocab):\n",
    "  '''Returns a numpy array containing the term document matrix for the input lines.\n",
    "  Inputs:\n",
    "    line_tuples: A list of tuples, containing the name of the document and \n",
    "    a tokenized line from that document.\n",
    "    document_names: A list of the document names\n",
    "    vocab: A list of the tokens in the vocabulary\n",
    "  Let m = len(vocab) and n = len(document_names).\n",
    "  Returns:\n",
    "    td_matrix: A mxn numpy array where the number of rows is the number of words\n",
    "        and each column corresponds to a document. A_ij contains the\n",
    "        frequency with which word i occurs in document j.\n",
    "  '''\n",
    "  vocab_to_id = dict(zip(vocab, range(0, len(vocab))))\n",
    "  docname_to_id = dict(zip(document_names, range(0, len(document_names))))\n",
    "  rows = len(vocab)\n",
    "  print(\"number of words =\" + str(rows))\n",
    "  cols = len(document_names)\n",
    "  print(\"number of documents = \" + str(cols))\n",
    "  td_matrix = np.zeros(shape=(rows, cols))\n",
    "  for tuple in line_tuples:\n",
    "        doc_name = tuple[0]\n",
    "        doc_id = docname_to_id[doc_name]\n",
    "        words = tuple[1]\n",
    "        for word in words:\n",
    "            vocab_id = vocab_to_id[word]\n",
    "            td_matrix[vocab_id, doc_id] += 1\n",
    "\n",
    "  return td_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo (and debugging) on a tiny corpus\n",
    "\n",
    "We illustrate with a tiny, manageable, understandable corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##Line Tuples\n",
      "(('cheese_and_crackers', ['cheese', 'and', 'crackers']), ('wine_and_cheese', ['wine', 'and', 'cheese']))\n",
      "##Doc Names\n",
      "['cheese_and_crackers', 'wine_and_cheese']\n",
      "##Vocab\n",
      "['wine', 'crackers', 'and', 'cheese']\n"
     ]
    }
   ],
   "source": [
    "line_tuples_demo = ((\"cheese_and_crackers\", \n",
    "                [\"cheese\", \"and\", \"crackers\"]), \n",
    "               (\"wine_and_cheese\", \n",
    "                [\"wine\", \"and\", \"cheese\"]), \n",
    "            \n",
    "        )\n",
    "\n",
    "vocab_demo = set()\n",
    "doc_names_demo = []\n",
    "\n",
    "def create_demo(line_tuples_demo):\n",
    "    for tup in line_tuples_demo:\n",
    "        doc_name = tup[0]\n",
    "        doc_names_demo.append(doc_name)\n",
    "        words = tup[1]\n",
    "        for word in words:\n",
    "            vocab_demo.add(word)\n",
    "    return list(vocab_demo), doc_names_demo\n",
    "\n",
    "vocab_demo, doc_names_demo = create_demo(line_tuples_demo)\n",
    "\n",
    "print(\"##Line Tuples\")\n",
    "print(line_tuples_demo)\n",
    "print(\"##Doc Names\")\n",
    "print(doc_names_demo)\n",
    "print(\"##Vocab\")\n",
    "print(vocab_demo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to create a term-document matrix just on this small example, to see exactly how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words =4\n",
      "number of documents = 2\n",
      "[[0. 1.]\n",
      " [1. 0.]\n",
      " [1. 1.]\n",
      " [1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(create_term_document_matrix(line_tuples_demo, doc_names_demo, vocab_demo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "What does this matrix represent?  (Please choose an element of the matrix, indicate its row/column, and explain its meaning.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your answer to Question 1\n",
    "\n",
    "This matrix represents the similarity of such documents to each other.\n",
    "\n",
    "Here, rows represent the words, columns represent the documents.\n",
    "\n",
    "For example, the first row \\[0. 1.\\] in the matrix means the numbers of occurance for \"wine\" in the document \"cheese_and_crackers\" and \"wine_and_cheese\". 0 means that \"wine\" doesn't occur in the document \"cheese_and_crackers\". 1 means that \"wine\" appears once in the document \"wine_and_cheese\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term-context matrix\n",
    "\n",
    "Now we are creating a term-context matrix.  You can change the size of the context window as an argument to the \"create_term_context_matrix\" function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_term_context_matrix(tuples, vocab, context_window_size=1):\n",
    "  '''Returns a numpy array containing the term context matrix for the input lines.\n",
    "  Inputs:\n",
    "    line_tuples: A list of tuples, containing the name of the document and \n",
    "    a tokenized line from that document.\n",
    "    vocab: A list of the tokens in the vocabulary\n",
    "  Let n = len(vocab).\n",
    "  Returns:\n",
    "    tc_matrix: A nxn numpy array where A_ij contains the frequency with which\n",
    "        word j was found within context_window_size to the left or right of\n",
    "        word i in any sentence in the tuples.\n",
    "  '''\n",
    "  vocab_to_id = dict(zip(vocab, range(0, len(vocab))))\n",
    "  tc_matrix = np.zeros(shape=(len(vocab), len(vocab)))\n",
    "  for tuple in tuples:\n",
    "        words = tuple[1]\n",
    "       #print(\"###### NEW LINE\")\n",
    "       # print(words)\n",
    "        # un-comment these to see what's happening!\n",
    "        \n",
    "        for i in range(0, len(words)):\n",
    "            target_word = str(words[i])\n",
    "            target_word_id = vocab_to_id[target_word]\n",
    "            left_context = words[i-context_window_size:i]\n",
    "            right_context = words[i+1:i+context_window_size + 1]\n",
    "            \n",
    "            #print(\"### Target Word\")\n",
    "            #print(target_word)\n",
    "            #print(\"Left Context\")\n",
    "            #print(left_context)\n",
    "            #print(\"Right Context\")\n",
    "            #print(right_context)\n",
    "            # you can un-comment these to see what's going on!  Also, try different \"Context window\" sizes.\n",
    "            \n",
    "            for context_word in left_context + right_context:\n",
    "                context_word_id = vocab_to_id[context_word]\n",
    "                tc_matrix[target_word_id, context_word_id] += 1\n",
    "            i += 1\n",
    "\n",
    " # print(\"Word IDs\") -- you can un-comment this if you want!\n",
    "  #for key in vocab_to_id.keys():\n",
    "   #     print(key + \": \" + str(vocab_to_id[key]))\n",
    "  return tc_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 1. 0. 2.]\n",
      " [0. 0. 2. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(create_term_context_matrix(line_tuples_demo, vocab_demo, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "What does this matrix represent?  Please choose a single cell of the matrix, indicate the row/column of this cell, and then explain its meaning.  (How do these numbers relate to \"cheese\", \"crackers\", \"and\", \"wine\")?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your answer to Question 2\n",
    "\n",
    "This matrix counts the occurance of word in the certain context with window size = 1. It can represent the similarity and relatedness of the words in the context.\n",
    "\n",
    "For example, a cell of matrix\\[3, 2\\] which has the value 2. Here, the row means the word \"cheese\" and the column means the word \"and\". It means that both \"cheese and\" and \"and cheese\" appear in the contexts. The word \"cheese\" and \"and\" have more closer relatedness or similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "Please change the \"context window\" to 2, and re-run the term-context matrix code.  What happens and why?  Again, please choose a single cell of the matrix and explain its meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 1. 0. 1.]\n",
      " [1. 1. 2. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(create_term_context_matrix(line_tuples_demo, vocab_demo, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your answer to Question 3\n",
    "\n",
    "The values of some cells are changed. Now only cell \\[1, 2\\] has the value 2.\n",
    "\n",
    "Since the window size is changed, the contexts for word counting are also changed.\n",
    "\n",
    "Now for the cell \\[3, 2\\] with the value 2, the row means word \"cheese\" and the column means word \"and\". Since now our window is 2, for word \"cheese\", it appears in the context \"and crackers\" and \"wine and\", so the co-occurence is 2. However, for the cell \\[2, 3\\] with the value 1, the row means word \"and\". The column means word \"cheese\". For word \"and\", it has the context \"crackers\" and \"cheese\". So the value is 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPMI matrix\n",
    "\n",
    "Now we'll create a PPMI matrix.  Please refer to the Jurafsky and Martin \"Vector Semantics\" chapter for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_PPMI_matrix(term_context_matrix, k=0.01):\n",
    "  '''Given a term context matrix, output a PPMI matrix.\n",
    "  See section 15.1 in the textbook.\n",
    "  Input:\n",
    "    term_context_matrix: A nxn numpy array, where n is\n",
    "        the numer of tokens in the vocab.\n",
    "  Returns: A nxn numpy matrix, where A_ij is equal to the\n",
    "     point-wise mutual information between the ith word\n",
    "     and the jth word in the term_context_matrix.\n",
    "  ''' \n",
    "  term_context_matrix += k # add constant k to each element to eliminate zeros (LaPlace smoothing)\n",
    "  total_sum = np.sum(term_context_matrix)\n",
    "  print(\"total sum: \" + str(total_sum))\n",
    "  Pij = term_context_matrix / total_sum\n",
    "  Pi = np.sum(term_context_matrix,axis=1) / total_sum\n",
    "  Pj = np.sum(term_context_matrix,axis=0) / total_sum\n",
    "  PiTimesPj = np.outer(Pi, Pj)\n",
    "  ppmi = np.where(np.log2(Pij / (PiTimesPj)) > 0, np.log2(Pij / (PiTimesPj)), 0)\n",
    "    # keep only the ppmi elements greater than 0.\n",
    "  return ppmi\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total sum: 8.16\n",
      "[[0.         0.         0.97198562 0.        ]\n",
      " [0.         0.         0.97198562 0.        ]\n",
      " [0.97198562 0.97198562 0.         0.99284021]\n",
      " [0.         0.         0.99284021 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(create_PPMI_matrix(create_term_context_matrix(line_tuples_demo, vocab_demo, context_window_size=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "Please choose a cell of this PPMI matrix, identify which row/column you are referring to, and explain what the value in this cell means as well as how it is calculated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your answer to Question 4\n",
    "\n",
    "The cell \\[2, 3\\] has the largest value 0.99284021. Here, the row is word \"cheese\" and column is word \"and\". \n",
    "\n",
    "The value 0.99284021 is the Positive Pointwise Mutual Information. It represents whether a context word is particularly informative about the target word.\n",
    "\n",
    "The calculation is based on the term-context matrix in question2. At first, we add a constant k = 0.01 to all the values in term-context martix _**M**_. Then, we get the sum _**S**_ of all the values in term-context matrix. Then we calculate _**Pij**_. _**Pij**_ is the value of term-context matrix _**M**_ divide the sum _**S**_. After that, we calculate the _**Pi**_ and _**Pj**_. _**Pi**_ is the array of sum of the values in each row. _**Pj**_ is the array of sum of the values in each column. Then, we calculate the outer product of  _**Pi**_ and _**Pj**_ as _**PiTimesPj**_. Finally, _**PPMI**_ is calculated by _**log2(Pij/PiTimesPj)**_. Here, we only keep the _**PPMI**_ whose value is larger than 0. If the value of _**PPMI**_ is smaller than 0, we set it as 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "\n",
    "Now please re-run the code that generates a term-context matrix and a PPMI matrix, but this time using a context window of 2 rather than 1.  For each of the matrices that you generate (the new term-context matrix and the new PPMI matrix), please again choose a cell and explain what it means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total sum: 10.16\n",
      "[[0.         0.         0.31625934 0.72654331]\n",
      " [0.         0.         0.31625934 0.72654331]\n",
      " [0.         1.30204549 0.         0.72654331]\n",
      " [1.28824497 0.31625934 0.32331341 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(create_PPMI_matrix(create_term_context_matrix(line_tuples_demo, vocab_demo, context_window_size=2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your answer to Question 5\n",
    "\n",
    "I choose the cell \\[2, 1\\]. It has the largest value 1.30204549. The row is word \"and\". The column is word \"crackers\". It means that \"and\" and \"cracker\" co-occur more than they were independant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "\n",
    "The code used to create the PPMI matrix is \"vectorized\".  Please explain what that means.  Feel free to Google around.\n",
    "\n",
    "If you can, please also write a for-loop which would calculate the value of each individual cell of the PPMI matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your answer to Question 6\n",
    "\n",
    "I think \"vectorized\" means that we calculate the PPMI based on vector and matrix by some linear algebra methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total sum: 8.159999999999998\n",
      "0.9719856238304037\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def create_PPMI_matrix_for_a_cell(term_context_matrix, m, n, k=0.01):\n",
    "    term_context_matrix += k # add constant k to each element to eliminate zeros (LaPlace smoothing)\n",
    "    total_sum = 0\n",
    "    for i in range(term_context_matrix.shape[0]):\n",
    "        for j in range(term_context_matrix.shape[1]):\n",
    "            total_sum += term_context_matrix[i, j]\n",
    "    print(\"total sum: \" + str(total_sum))\n",
    "    Pij = term_context_matrix[m, n] / total_sum \n",
    "    \n",
    "    Pi = 0\n",
    "    for i in range(term_context_matrix.shape[1]):\n",
    "        Pi += term_context_matrix[m, i]\n",
    "    Pi = Pi / total_sum\n",
    "    Pj = 0\n",
    "    for i in range(term_context_matrix.shape[0]):\n",
    "        Pj += term_context_matrix[i, n]\n",
    "    Pj = Pj / total_sum\n",
    "    PiTimesPj = Pi * Pj\n",
    "   \n",
    "    ppmi = math.log2(Pij / PiTimesPj) if math.log2(Pij / PiTimesPj) > 0 else 0\n",
    "    \n",
    "    return ppmi\n",
    "\n",
    "print(create_PPMI_matrix_for_a_cell(create_term_context_matrix(line_tuples_demo, vocab_demo, context_window_size=1), 1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With my for-loop method which would calculate the value of each individual cell of the PPMI matrix, you just need to give the term-context matrix and the row index as well as the column index of the cell to the function, it can give you the value of PPMI for that cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf-idf matrix\n",
    "\n",
    "Now we'll create a tf-idf matrix: a matrix indicating, for each word, its tf-idf among our collection of documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "term document matrix\n",
      "number of words =4\n",
      "number of documents = 2\n",
      "[[0. 1.]\n",
      " [1. 0.]\n",
      " [1. 1.]\n",
      " [1. 1.]]\n",
      "tf-idf matrix\n",
      "number of words =4\n",
      "number of documents = 2\n",
      "[[0.         0.69314718]\n",
      " [0.69314718 0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "def create_tf_idf_matrix(term_document_matrix):\n",
    "  '''Given the term document matrix, output a tf-idf weighted version.\n",
    "  Input:\n",
    "    term_document_matrix: Numpy array where each column represents a document \n",
    "    and each row, the frequency of a word in that document.\n",
    "\n",
    "  Returns:\n",
    "    A numpy array with the same dimension as term_document_matrix, where\n",
    "    A_ij is weighted by the inverse document frequency of document h.\n",
    "  '''\n",
    "  N_docs = term_document_matrix.shape[1] # number of columns of the matrix = number of documents\n",
    "  idf = np.count_nonzero(term_document_matrix, axis=1) # this calculates doc frequency of each term\n",
    "  tfidf = (np.log(N_docs / idf) * term_document_matrix.T).T\n",
    "  return tfidf\n",
    "\n",
    "print(\"term document matrix\")\n",
    "print(create_term_document_matrix(line_tuples_demo, doc_names_demo, vocab_demo))\n",
    "print(\"tf-idf matrix\")\n",
    "print(create_tf_idf_matrix((create_term_document_matrix(line_tuples_demo, doc_names_demo, vocab_demo))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7\n",
    "\n",
    "Please choose a cell in this matrix and explain what the value in it means and how it is calculated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your answer to Question 7\n",
    "\n",
    "I choose the cell \\[0, 1\\] with the value 0.69314718. It means the frequency of word among the collection of documents. Also, it means how important a word is to a document in a collection or corpus. Since \"cheese\" and \"and\" appear in both two documents, so their value is 0.\n",
    "\n",
    "To calculate the value, first, we calculate the tf-idf. Then, we multiply the log of tf-idf and the transpose of term-document matrix. Finally, we get the transpose of the result in last step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we write code to compute the cosine similarity between vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7142857142857143\n",
      "1.0\n",
      "0.9746318461970761\n",
      "0.8536086925965384\n"
     ]
    }
   ],
   "source": [
    "from scipy import spatial\n",
    "def compute_cosine_similarity(vector1, vector2):\n",
    "  '''Computes the cosine similarity of the two input vectors.\n",
    "\n",
    "  Inputs:\n",
    "    vector1: A nx1 numpy array\n",
    "    vector2: A nx1 numpy array\n",
    "\n",
    "  Returns:\n",
    "    A scalar similarity value.\n",
    "  '''\n",
    "  \n",
    "  return 1 - spatial.distance.cosine(vector1, vector2)\n",
    "\n",
    "print(compute_cosine_similarity([1,2,3], [3,2,1]))\n",
    "print(compute_cosine_similarity([1,2,3], [10,20,30]))\n",
    "print(compute_cosine_similarity([1,2,3], [4,5,6]))\n",
    "print(compute_cosine_similarity([1,2,3], [3,30,300]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8\n",
    "\n",
    "What is cosine similarity?   What is the range of possible values that it can yield?  (Feel free to read online about this to come up with your answer.)  Can you explain the outputs printed above on an intuitive or conceptual level?  What does a \"higher\" number mean?  What does a \"lower\" number mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your anwer to Question 8\n",
    "\n",
    "Cosine similarity is a measure of similarity between two non-zero vectors of an inner product space that measures the cosine of the angle between them. The range of possible values is from -1 to 1. \n",
    "\n",
    "The outputs printed above show the similarity of each pair of vectors. Higher number means that the two vectors are more similar. Lower number means two vectors are more different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will write a function which ranks the similarity of all documents to a single target document.   In the spirit of the Honor Code, I acknowledge that some of this code is from Stack Exchange."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words =4\n",
      "number of documents = 2\n",
      "[[0. 1.]\n",
      " [1. 0.]\n",
      " [1. 1.]\n",
      " [1. 1.]]\n",
      "Index of the document most similar document to Doc1 is.... \n",
      "0\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "\n",
    "def rank_docs(target_doc_index, term_document_matrix):\n",
    "  ''' Ranks the similarity of all of the plays to the target play.\n",
    "  Inputs:\n",
    "    target_doc_index: The integer index of the doc we want to compare all others against.\n",
    "    term_document_matrix: The term-document matrix as a mxn numpy array.\n",
    "\n",
    "  Returns:\n",
    "    The integer index of the doc with the least (cosine) distance to the doc indexed by target_doc_index.\n",
    "  '''\n",
    "\n",
    "  target = term_document_matrix.T[target_doc_index]\n",
    " # print(\"target\")\n",
    " # print(target) you can uncomment these if you want to see it\n",
    "  vectors = list(term_document_matrix.T)\n",
    " # print(\"vectors\")\n",
    " # print(vectors)\n",
    "  distances = distance.cdist([target], vectors, \"cosine\")[0]\n",
    "  min_index = np.argpartition(distances, 1)[1] # this gets the SECOND-closest vector\n",
    "                # because the CLOSEST vector is always the target vector itself.\n",
    "\n",
    "  return min_index\n",
    "\n",
    "\n",
    "\n",
    "term_doc_matrix = create_term_document_matrix(line_tuples_demo, doc_names_demo, vocab_demo)\n",
    "print(term_doc_matrix)\n",
    "\n",
    "print(\"Index of the document most similar document to Doc1 is.... \")\n",
    "print(rank_docs(1, term_doc_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9\n",
    "\n",
    "What happened here?  What does \"0\" mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your answer to Question 9\n",
    "\n",
    "It calculates the second closest document for the doc1 based on the consine similarity. Here, the first closest document is always the target document itself. \n",
    "0 means that the closest document to doc1 is doc0 except doc1 itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll write code to try to find the most similar WORD to another word, based on the term-context-matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 1. 0. 2.]\n",
      " [0. 0. 2. 0.]]\n",
      "Index of the word most similar to Word3 is.... \n",
      "target\n",
      "[0. 0. 2. 0.]\n",
      "vectors\n",
      "[array([0., 0., 1., 0.]), array([0., 0., 1., 0.]), array([1., 1., 0., 2.]), array([0., 0., 2., 0.])]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def rank_words(target_word_index, term_context_matrix):\n",
    "  ''' Ranks the similarity of all of the words to the target word.\n",
    "\n",
    "  Inputs:\n",
    "    target_word_index: The index of the word we want to compare all others against.\n",
    "    term_context_matrix: Numpy matrix where the ith row represents a vector embedding of the ith word.\n",
    "\n",
    "  Returns:\n",
    "    A length-n list of integer word indices, ordered by decreasing (cosine) similarity to the \n",
    "    target word indexed by word_index\n",
    "  '''\n",
    "\n",
    "  target = term_context_matrix.T[target_word_index]\n",
    "  print(\"target\")\n",
    "  print(target) #you can uncomment these if you want to see it\n",
    "  vectors = list(term_context_matrix.T)\n",
    "  print(\"vectors\")\n",
    "  print(vectors)\n",
    "  distances = distance.cdist([target], vectors, \"cosine\")[0]\n",
    "  min_index = np.argpartition(distances, 1)[1] # this gets the SECOND-closest vector\n",
    "                # because the CLOSEST vector is always the target vector itself.\n",
    "\n",
    "  return min_index\n",
    "\n",
    "\n",
    "\n",
    "term_context_matrix = create_term_context_matrix(line_tuples_demo, vocab_demo, 1)\n",
    "print(term_context_matrix)\n",
    "\n",
    "print(\"Index of the word most similar to Word3 is.... \")\n",
    "print(rank_words(3, term_context_matrix))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 10\n",
    "\n",
    "What does this mean?  What is Word3?  What is Word1?  Why is Word1 similar to Word3?  (How does all this relate to cheese, crackers, and wine?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your answer to Question 10\n",
    "\n",
    "It calculates the closest word to the target word except itself. \n",
    "Word3 is \"cheese\".\n",
    "Word1 is \"crackers\".\n",
    "Since the consine similarity of vector for \"cheese\" appear in context and vector for \"crackers\" is biggest, word1 is similar to word3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Question 11\n",
    "\n",
    "Why is it useful to run our code on a tiny corpus instead of a much bigger one?  When you do big data projects, do you also practice and debug on a tiny subset of the data?  Why or why not?\n",
    "\n",
    "Also -- this homework uses Numpy arrays (mainly because those were used in the source material that this is adapted from), but it would actually be better to use Pandas dataframes where it is easier to label rows and columns.  If you are feeling ambitious, can you make a Pandas array of our term-by-document matrix (for doc1=\"cheese and crackers\", doc2= \"wine and cheese\") with the rows and columns labeled in an easy-to-read manner?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your answer to Question 11\n",
    "\n",
    "I think it can save more time and easy to debug on the tiny corpus than much bigger one.\n",
    "I also practice and debug on a tiny subset of the data to develop the new functions. It's better for me to understand the algorithms and solve the bugs. If my function can work on the smaller dataset, it can work on a bigger dataset, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words =4\n",
      "number of documents = 2\n",
      "          cheese_and_crackers  wine_and_cheese\n",
      "wine                      0.0              1.0\n",
      "crackers                  1.0              0.0\n",
      "and                       1.0              1.0\n",
      "cheese                    1.0              1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "term_document_matrix = create_term_document_matrix(line_tuples_demo, doc_names_demo, vocab_demo)\n",
    "term_document_dataframe = pd.DataFrame(data=term_document_matrix, \n",
    "                                       index=['wine', 'crackers', 'and', 'cheese'], \n",
    "                                       columns=['cheese_and_crackers', 'wine_and_cheese'])\n",
    "print(term_document_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 12\n",
    "\n",
    "Please read this article: https://www.quantamagazine.org/machines-beat-humans-on-a-reading-test-but-do-they-understand-20191017 and then indicate FIVE substantive facts that you learned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your answer to Question 12\n",
    "\n",
    "1. Traditional machine learning models cannot solve GLUE since they cannot learn anything substantial about language itself. However, deep learning model like BERT can solve GLUE to some extent and has good performance.\n",
    "2. A neural network pretrained with word embeddings is still blind to the meaning of words at the sentence level. For example,‘a man bit the dog’ and ‘a dog bit the man’ are exactly the same thing. Then the researches tried to equip the network with richer rulebooks — not just for vocabulary, but for syntax and context as well. They proposed the language modeling and with fine-tuning, it has the great performance on the GLUE.\n",
    "3. The three ingredients of BERT are a deep pretrained language model, attention and bidirectionality. They existed independently before BERT. However, the scientists in Google combined them together in such a powerful way and created the innovative BERT.\n",
    "4. BERT’s “pie crust” incorporates a number of structural design decisions that affect how well it works. These include the size of the neural network being baked, the amount of pretraining data, how that pretraining data is masked and how long the neural network gets to train on it. So there are still many researchers tweaking these design decisions to make their BERT models work better. \n",
    "5. BERT’s high performance on certain GLUE tasks might be attributed to spurious cues in the training data for those tasks. One way to encourage progress toward robust understanding is to focus not just on building a better BERT, but also on designing better benchmarks and training data that lower the possibility of Clever Hans–style cheating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 13\n",
    "\n",
    "Please try out AllenNLP's \"textual entailment\" tool: https://demo.allennlp.org/textual-entailment\n",
    "\n",
    "How do you think this tool was built?  How good is it?  What types of sentence pairs can it handle well, what pairs does it struggle with, and why do you think that is?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your answer to Question 13\n",
    "\n",
    "Textual entailment is the task of determining whether two natural language sentences are (i) contradicting each other, (ii) not related, or whether (iii) the first sentence (called premise) entails the second sentence (called hypothesis). Via the paper, I learned that the textual entailment uses (i) conditional encoding via two LSTM, one over the premise and one over the hypothesis conditioned on the representation of the premise (ii) attention only based on the last output vector and (iii) word-by-word attention based on the output vectors of the hypothesis.\n",
    "\n",
    "The performance is good. I tried all the sample pairs of premise and hypothesis. The text entailment tool gave the correct answers.\n",
    "\n",
    "For the pairs with common words like \"Two women are wandering along the shore drinking iced tea.\" and \"Two women are sitting on a blanket near some rocks talking about politics.\", it handles well.\n",
    "\n",
    "For the pairs without the common words like \"I am tired\" and \"I am going to bed\". It struggles and doesn't give the correct answer based on human understanding.\n",
    "\n",
    "I think this model still depends on the vocabulary and syntax of sentences a lot. So for those pairs without common words, the model cannot catch the similarity of words and perform poorly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 14\n",
    "\n",
    "What do you think you will do for your final project in this class?  Please write a few sentences so that I can give you some feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your answer to Question 14\n",
    "\n",
    "For my final project, I think that spell checker is really interesting and useful in our daily life. I want to improve the Norvig’s spell-checker by considering the candidate words of more edit distane, 2-gram or 3-gram for catching more complex multi-typos, hamming distance and word-frequency model based chooser for suggestions and so on. Also, I want to explore some machine learning methods to check the grammer errors in the sentence. Finally, I will try to build the user interface for my spell checker to show it in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
