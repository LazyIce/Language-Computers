{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your name: Bin Xie\n",
    "\n",
    "## Names of people you worked with\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Boolean Search\n",
    "\n",
    "adapted from Lucas Champollion and Frans Adriaans)\n",
    "\n",
    "Here are 5 documents, from Wikipedia, about Star Wars.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#############doc1#############\n",
      "Frank  Oz  provided  Yoda's   voice  with  each  film  and  spent  his  skills  to  be  a  puppeteer  in  the  original  trilogy  and  Star  Wars  Episode  I:  The  Phantom  Menace.  For  the  latter,  in  some  walking  scenes,  Warwick  Davis  incarnated  Yoda  as  well.  For  the  radio  dramatizations  of  The  Empire  Strikes  Back  and  Return  of  the  Jedi,  Yoda  was  voiced  by  John  Lithgow,  while  Tom  Kane  voiced  him  in  the  Clone  Wars  animated  series,  several  video  games,  and  the  new  series  Star  Wars:  The  Clone  Wars.\n",
      "#############doc2#############\n",
      "Luke  Skywalker  lives  a  humdrum  existence  on  Tatooine  with  his  Uncle  Owen  and  Aunt  Beru  that  have  kept  his  father's  true  history  a  secret  from  him.  He  initially  wants  to  join  the  Imperial  Academy  to  become  a  pilot  with  his  childhood  friend  Biggs  Darklighter,  but  is  held  back  by  his  uncle  who  ostensibly  needs  his  help  on  the  moisture  farm  (while  it  was  to  hopefully  prevent  Luke  from  following  his  father's  path).  He  takes  his  first  steps  toward  his  destiny  when  he  finds  the  two  droids  C-3PO  and  R2-D2.  After  delivering  R2-D2's  message  to  hermit  Ben  Kenobi,  Ben  tells  Luke  that  his  father  was  a  Jedi  and  presents  him  with  his  father's  lightsaber  and  then  tells  him  that  his  father  was  murdered  by  a  traitorous  Jedi.  Ben  offers  to  take  Luke  to  the  planet  Alderaan  and  train  him  in  the  ways  of  the  Force,  but  Luke  rejects  his  offer.\n",
      "#############doc3#############\n",
      "When  the  Empire  attacks  the  Rebel  base,  Solo  transports  Chewbacca,  along  with  Princess  Leia  and  C-3PO  to  Cloud  City  where  his  old  friend  (and  Cloud  City  administrator)  Lando  Calrissian  operates  to  hide  from  Imperial  agents.  When  bounty  hunter  Boba  Fett  tracks  the  Falcon  to  Cloud  City,  Darth  Vader  forces  Calrissian  to  help  capture  Solo  sealed  in  carbonite  for  delivery  to  Jabba  the  Hutt.  But  Lando  frees  Vader's  other  captives  and  they  may  rescue  Solo  but  are  too  late  as  Fett  escapes  with  Solo's  frozen  body.\n",
      "#############doc4#############\n",
      "In  her  first  appearance  in  Star  Wars  Episode  IV:  A  New  Hope,  Princess  Leia  is  introduced  as  the  Princess  of  Alderaan  and  a  member  of  the  Imperial  Senate.  Darth  Vader  captures  her  on  board  the  ship  Tantive  IV,  where  she  is  acting  as  a  spy  for  the  Rebel  Alliance.  He  accuses  her  of  being  a  traitor  and  demands  to  know  the  location  of  the  secret  technical  plans  of  the  Death  Star,  the  Galactic  Empire's  most  powerful  weapon.  Unknown  to  Vader,  the  young  Senator  has  hidden  the  plans  inside  the  Astromech  droid  R2-D2  and  has  sent  it  to  find  Jedi  Master  Obi-Wan  Kenobi  on  the  nearby  planet  of  Tatooine.\n",
      "#############doc5#############\n",
      "Three  years  later  in  Star  Wars  Episode  V:  The  Empire  Strikes  Back,  Lord  Vader  leads  an  Imperial  starfleet  in  pursuit  of  the  Rebels.  Intent  on  turning  Luke  to  the  dark  side,  Vader  captures  Princess  Leia,  Han  Solo,  Chewbacca  and  C-3PO  on  Cloud  City  to  use  them  as  bait  for  Luke.  During  a  lightsaber  duel,  Vader  cuts  off  Luke's  right  hand  and  reveals  that  he  is  Luke's  father\n"
     ]
    }
   ],
   "source": [
    "doc1  =  \"Frank  Oz  provided  Yoda's   voice  with  each  film  and  spent  his  skills  to  be  a  puppeteer  in  the  original  trilogy  and  Star  Wars  Episode  I:  The  Phantom  Menace.  For  the  latter,  in  some  walking  scenes,  Warwick  Davis  incarnated  Yoda  as  well.  For  the  radio  dramatizations  of  The  Empire  Strikes  Back  and  Return  of  the  Jedi,  Yoda  was  voiced  by  John  Lithgow,  while  Tom  Kane  voiced  him  in  the  Clone  Wars  animated  series,  several  video  games,  and  the  new  series  Star  Wars:  The  Clone  Wars.\"\n",
    "\n",
    "doc2  =  \"Luke  Skywalker  lives  a  humdrum  existence  on  Tatooine  with  his  Uncle  Owen  and  Aunt  Beru  that  have  kept  his  father's  true  history  a  secret  from  him.  He  initially  wants  to  join  the  Imperial  Academy  to  become  a  pilot  with  his  childhood  friend  Biggs  Darklighter,  but  is  held  back  by  his  uncle  who  ostensibly  needs  his  help  on  the  moisture  farm  (while  it  was  to  hopefully  prevent  Luke  from  following  his  father's  path).  He  takes  his  first  steps  toward  his  destiny  when  he  finds  the  two  droids  C-3PO  and  R2-D2.  After  delivering  R2-D2's  message  to  hermit  Ben  Kenobi,  Ben  tells  Luke  that  his  father  was  a  Jedi  and  presents  him  with  his  father's  lightsaber  and  then  tells  him  that  his  father  was  murdered  by  a  traitorous  Jedi.  Ben  offers  to  take  Luke  to  the  planet  Alderaan  and  train  him  in  the  ways  of  the  Force,  but  Luke  rejects  his  offer.\"\n",
    "\n",
    "doc3 =  \"When  the  Empire  attacks  the  Rebel  base,  Solo  transports  Chewbacca,  along  with  Princess  Leia  and  C-3PO  to  Cloud  City  where  his  old  friend  (and  Cloud  City  administrator)  Lando  Calrissian  operates  to  hide  from  Imperial  agents.  When  bounty  hunter  Boba  Fett  tracks  the  Falcon  to  Cloud  City,  Darth  Vader  forces  Calrissian  to  help  capture  Solo  sealed  in  carbonite  for  delivery  to  Jabba  the  Hutt.  But  Lando  frees  Vader's  other  captives  and  they  may  rescue  Solo  but  are  too  late  as  Fett  escapes  with  Solo's  frozen  body.\"\n",
    "\n",
    "doc4  =  \"In  her  first  appearance  in  Star  Wars  Episode  IV:  A  New  Hope,  Princess  Leia  is  introduced  as  the  Princess  of  Alderaan  and  a  member  of  the  Imperial  Senate.  Darth  Vader  captures  her  on  board  the  ship  Tantive  IV,  where  she  is  acting  as  a  spy  for  the  Rebel  Alliance.  He  accuses  her  of  being  a  traitor  and  demands  to  know  the  location  of  the  secret  technical  plans  of  the  Death  Star,  the  Galactic  Empire's  most  powerful  weapon.  Unknown  to  Vader,  the  young  Senator  has  hidden  the  plans  inside  the  Astromech  droid  R2-D2  and  has  sent  it  to  find  Jedi  Master  Obi-Wan  Kenobi  on  the  nearby  planet  of  Tatooine.\"\n",
    "\n",
    "doc5  =  \"Three  years  later  in  Star  Wars  Episode  V:  The  Empire  Strikes  Back,  Lord  Vader  leads  an  Imperial  starfleet  in  pursuit  of  the  Rebels.  Intent  on  turning  Luke  to  the  dark  side,  Vader  captures  Princess  Leia,  Han  Solo,  Chewbacca  and  C-3PO  on  Cloud  City  to  use  them  as  bait  for  Luke.  During  a  lightsaber  duel,  Vader  cuts  off  Luke's  right  hand  and  reveals  that  he  is  Luke's  father\"\n",
    "\n",
    "\n",
    "#  We're  going  to  store  these  documents  in  a  dictionary,  which  makes  them  easier  to  work  with:\n",
    "docs  =  {'doc1':doc1, \n",
    "        'doc2': doc2, \n",
    "        'doc3': doc3, \n",
    "        'doc4': doc4, \n",
    "        'doc5': doc5}\n",
    "\n",
    "# let's just print them out again to be sure...\n",
    "#(\"sorted\" puts them in order; without that, dictionaries are un-sorted)\n",
    "\n",
    "for key in sorted(docs.keys()):\n",
    "    print(\"#############\" + key + \"#############\")\n",
    "    print(docs[key])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following \"Boolean Search\" function takes in 3 arguments\n",
    "\n",
    "\n",
    "---   the  operator  (\"and\"  or  \"or\")\n",
    "\n",
    "-- a  list  of  terms  to  search  for  ([\"Term1\",\"Term2\"...\"TermN\"])\n",
    "\n",
    "-- the  document  collection  to  search  through  (in  this  case  our  dictionary  called  docs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  boolean(comparison,terms,docs):\n",
    "    #  invert  dictionary        \n",
    "    docs_inverse  = {value.lower():key for key,value  in docs.items()} \n",
    "    relevant  =  [] # a list of the documents that are relevant; we start with an empty list.\n",
    "    if  comparison  ==\"or\":\n",
    "        for  d  in  docs:               \n",
    "            document  =  docs[d].lower()     \n",
    "            rel  =  False\n",
    "            for  term  in  terms:\n",
    "                if term.lower() in  document: \n",
    "                    rel  =  True\n",
    "                    if  rel  == True:\n",
    "                        relevant.append(docs_inverse[document])\n",
    "    if  comparison  ==\"and\":\n",
    "        for  d  in  docs: \n",
    "            document  =  docs[d].lower()  \n",
    "            rel  =  True\n",
    "            for  term  in  terms:\n",
    "                if not (term.lower() in  document): \n",
    "                    rel  =  False\n",
    "            if  rel  == True:\n",
    "                relevant.append(docs_inverse[document]) \n",
    "    return  relevant\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['doc2', 'doc3']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boolean(\"and\",  [\"force\"],  docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "What just happened, in prose?  Why did we get back \"doc2\" and \"doc3\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your answer to Question 1\n",
    "\n",
    "The program finds the relevant docs which contain word \"force\".\n",
    "We get the answer \"doc2\" and \"doc3\" because only doc2 and doc3 contain the word \"force\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Now let's run the code again, but with two search terms (\"force\" and \"Leia\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['doc3']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boolean(\"and\",  [\"force\", \"Leia\"],  docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What just happened here  Why do we only get \"doc3\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your answer to Question 2\n",
    "\n",
    "This program finds the relevant docs which contain both words \"force\" and \"Leia\".\n",
    "We get the answer \"doc3\" because only doc3 contains both \"force\" and \"Leia\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "Now let's run it again, this time with \"or\" instead of \"and\" as the first argument.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['doc2', 'doc3', 'doc3', 'doc4', 'doc5']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boolean(\"or\",  [\"force\", \"Leia\"],  docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What just happened here?  Why do we get all of these documents back?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your answer to Question 3\n",
    "\n",
    "This program finds the relevant docs which contain either word \"force\" or word \"Leia\" or both of them.\n",
    "We get all these docs back because doc2 contains the word \"force\". And, doc3 contains the word \"force\" and word \"Leia\". What's more, doc4 and doc5 contain the word \"Leia\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Vector Similarity\n",
    "\n",
    "(again, thanks to Lucas Champollion and Frans Adriaans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The  boolean  search  demonstrated  above  requires  exact  matches  and  returns  all  matching  documents.  Vector  space  models  instead  allow  partial  matching  and,  as  implemented  below,  allow  us  to  rank  the  returned  documents.  Informally,  this  model  generates  a  numeric  representation  of  each  document  and  the  query  and  computes  similarity  as  a  number  between  0  and  1.  A  document  with  a  low  score  contains  few  of  the  search  terms,  while  a  document  with  a  higher  score  contains  more  search  terms.  This  measure  of  similarity  also  normalizes  for  document  length  (accounts  for  differing  document  length  when  computing  scores).  Without  this  feature,  longer  documents  would  generally  score  higher  simply  because  they  contain  more  words  in  total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Square  root  and  log  functions  needed  for  calculations:\n",
    "\n",
    "from  math  import  sqrt,  log\n",
    "\n",
    "#  Dot  product  makes  things  a  lot  simpler  \n",
    "# (technically  the  notebook  setup  imports  this  for  us  by  default)\n",
    "from  numpy  import  dot\n",
    "\n",
    "#  cosine  similiarity:\n",
    "def  cosine_sim(d,q):       \n",
    "    sim_un  =  dot(d,q)        \n",
    "    norm  =  sqrt(dot(d,d))*sqrt(dot(q,q))+1e-16\n",
    "    return round(sim_un/norm,3)\n",
    "\n",
    "#  Generate  weight  vectors  for  each  document:\n",
    "def  document_vectors(terms,docs):  \n",
    "    terms  = [term.lower() for  term  in  terms]\n",
    "    #  define  some  useful  values  used  repeatedly  in  calculations  (N  and  n  from  above):        \n",
    "    N  =  len(docs)        \n",
    "    counts  =  [sum([1.0 for  d  in  docs  if  term  in docs[d]])  for  term  in  terms]\n",
    "    #  pre-calculate  the  scaling  term  for  each  weight:        \n",
    "    scaling  =  [log(N/(n+1e-16),2)  for  n  in  counts]\n",
    "    #  weight  vectors  for  each  document:        \n",
    "    weights  =  {d:[1e-16]*len(terms)  for  d  in  docs}\n",
    "    #  actual  calculations\n",
    "    for  j  in  docs:                \n",
    "        document  =  docs[j].lower().split()    \n",
    "        for  i  in range(len(terms)):                       \n",
    "            frequency  = document.count(terms[i])/float(len(terms))          \n",
    "            weights[j][i]  =  frequency*scaling[i]\n",
    "    #  return  weight  vectors:\n",
    "    return  weights\n",
    "\n",
    "\n",
    "#  function  that  will  take  a  list  of  terms  and  documents  and \n",
    "# return  a  ranking  by  relevance\n",
    "def  vector_query(terms,docs):        \n",
    "    vectors  =  document_vectors(terms,docs)\n",
    "    similarities  =  {}        \n",
    "    q  =  [1]*len(terms)\n",
    "    for  d  in  docs:\n",
    "        similarities[d]  =  cosine_sim(vectors[d],q)\n",
    "    return  similarities\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You  can  do  Vector  Space  retrieval  by  calling  the  above  function  (called  vector_query).  When  calling  the  function,  you  need  to  give  it  two  arguments:\n",
    "\n",
    "-- a  list  of  search  terms  ([\"Term1\",\"Term2\"...\"TermN\"])\n",
    "\n",
    "-- the  document  collection  to  search  through  (in  this  case  our  dictionary  called  docs)\n",
    "\n",
    "You  can  either  type  the  seach  term  list  by  hand,  or  use  use  nltk's  \n",
    "word_tokenize  method (which maps a string to a list of words). \n",
    "\n",
    "In  the  latter  case,  you  will  need  to  provide  the  query  as  a  single  string,  and  it  will  return  the  query  as  a  tokenized  list.\n",
    "\n",
    "We'll try  running  the  following  example: \"Jedi  master\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc4 1.0\n",
      "doc2 0.707\n",
      "doc1 0.0\n",
      "doc3 0.0\n",
      "doc5 0.0\n"
     ]
    }
   ],
   "source": [
    "#  Import  the  \"word_tokenize\"  method:\n",
    "from  nltk  import  word_tokenize\n",
    "\n",
    "#  Specify  the  query  as  a  list  of  search  terms:\n",
    "terms  =  word_tokenize(\"Jedi  master\")\n",
    "\n",
    "#  Do  vector  space  retrieval  on  the  search  query:\n",
    "results  =  vector_query(terms,  docs)\n",
    "\n",
    "#  Print  a  ranked  list  of  results:\n",
    "for  doc  in sorted(results, key=results.get, reverse=True):\n",
    "    print(doc,  results[doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "What just happened here?  What do these numbers represent?  Why does Doc4 get the highest number?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your answer to Question 4\n",
    "\n",
    "This program calculates the score of each document based on the document length and the number of terms each document matches.\n",
    "\n",
    "These numbers represent the relevance of the docs and terms. This number is calculated by the frequency of terms in doc and the length of doc. If the length of the doc is smaller and the frequency of the terms is larger in this doc, the number of score can be higher. With the same length, the docs have higher score number can find more terms.\n",
    "\n",
    "Doc4 gets the highest number because it can find most terms after normalizing the length of doc among all docs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "\n",
    "Consider  the  following  queries:\n",
    "\n",
    "1.\"Luke,  I  am  your  father\"\n",
    "\n",
    "2.\"May  the  force  be  with  you\"\n",
    "\n",
    "Write  down  for  each  query  which  document  you  think  is  most  relevant.  Simply  use  your  own  intuitions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your answer to Question 5\n",
    "\n",
    "For the query1, doc5 is most relavant.\n",
    "For the query2, doc3 is most relavant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "Use  the  Vector  Space  retrieval  model  to  determine  which  documents  are  relevant  for  each  query.  \n",
    "\n",
    "Does  the  ranked  list  match  your  intuitions?  Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc5 0.418\n",
      "doc2 0.412\n",
      "doc1 0.0\n",
      "doc3 0.0\n",
      "doc4 0.0\n"
     ]
    }
   ],
   "source": [
    "terms = word_tokenize(\"Luke, I am your father\")\n",
    "results = vector_query(terms,  docs)\n",
    "for doc in sorted(results, key=results.get, reverse=True):\n",
    "    print(doc, results[doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc3 0.563\n",
      "doc1 0.408\n",
      "doc2 0.408\n",
      "doc4 0.0\n",
      "doc5 0.0\n"
     ]
    }
   ],
   "source": [
    "terms = word_tokenize(\"May the force be with you\")\n",
    "results = vector_query(terms,  docs)\n",
    "for doc in sorted(results, key=results.get, reverse=True):\n",
    "    print(doc, results[doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your answer to Question 6\n",
    "\n",
    "Yes, the ranked list match my intuitions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "Would  the  removal  of  stop  words  in  the  query  improve  the  retrieval  results?  Come  up  with  a  list  of  words  to  remove  and  show  the  results  when  you  leave  out  these  words  from  the  query.\n",
    "\n",
    "You can read more about stop-words on Wikipedia: https://en.wikipedia.org/wiki/Stop_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your answer to Question 7\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "\n",
    "Which  terms  would  you  need  to  add  to  doc2  to  make  it  the  highest  ranked  document  for  the  query  \"May  the  force  be  with  you\"?  Insert  terms  in  the  document  text  (you  should  re-define  it  in  the  cell  below  rather  than  altering  the  original  definition  above)  and  show  that these changes do make it the highest-ranked document for that query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Luke  Skywalker  lives  a  humdrum  existence  on  Tatooine  with  his  Uncle  Owen  and  Aunt  Beru  that  have  kept  his  father's  true  history  a  secret  from  him.  He   initially  wants  to  join  the  Imperial  Academy  to  become  a  pilot  with  his  childhood  friend  Biggs  Darklighter,  but  is  held  back  by  his  uncle  who  ostensibly  needs  his  help  on  the  moisture  farm  (while  it  was  to  hopefully  prevent  Luke  from  following  his  father's  path).  He  takes  his  first  steps  toward  his  destiny  when  he  finds  the  two  droids  C-3PO  and  R2-D2.  After  delivering  R2-D2's  message  to  hermit  Ben  Kenobi,  Ben  tells  Luke  that  his  father  was  a  Jedi  and  presents  him  with  his  father's  lightsaber  and  then  tells  him  that  his  father  was  murdered  by  a  traitorous  Jedi.  Ben  offers  to  take  Luke  to  the  planet  Alderaan  and  train  him  in  the  ways  of  the  Force,  but  Luke  rejects  his  offer.\n"
     ]
    }
   ],
   "source": [
    "doc2  =  \"Luke  Skywalker  lives  a  humdrum  existence  on  Tatooine  with  his  Uncle  Owen  and  Aunt  Beru  that  have  kept  his  father's  true  history  a  secret  from  him.  He   initially  wants  to  join  the  Imperial  Academy  to  become  a  pilot  with  his  childhood  friend  Biggs  Darklighter,  but  is  held  back  by  his  uncle  who  ostensibly  needs  his  help  on  the  moisture  farm  (while  it  was  to  hopefully  prevent  Luke  from  following  his  father's  path).  He  takes  his  first  steps  toward  his  destiny  when  he  finds  the  two  droids  C-3PO  and  R2-D2.  After  delivering  R2-D2's  message  to  hermit  Ben  Kenobi,  Ben  tells  Luke  that  his  father  was  a  Jedi  and  presents  him  with  his  father's  lightsaber  and  then  tells  him  that  his  father  was  murdered  by  a  traitorous  Jedi.  Ben  offers  to  take  Luke  to  the  planet  Alderaan  and  train  him  in  the  ways  of  the  Force,  but  Luke  rejects  his  offer.\"\n",
    "\n",
    "docs['doc2']  =  doc2\n",
    "\n",
    "print(doc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your answer to Question 8\n",
    "\n",
    "(PLEASE PUT YOUR ANSWER HERE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Precision and Recall\n",
    "\n",
    "### Question 9\n",
    "\n",
    "You can read more about precision and recall here: https://en.wikipedia.org/wiki/Precision_and_recall\n",
    "\n",
    "Please Google yourself (your name, in quotes).\n",
    "\n",
    "Please estimate the PRECISION of these results.\n",
    "\n",
    "Please estimate the RECALL of these results.  (Why might it be harder to estimate RECALL than PRECISION?)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your answer to Question 9\n",
    "\n",
    "(PLEASE PUT YOUR ANSWER HERE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10\n",
    "\n",
    "What might be some advantages and disadvantages of having high or low precision/recall for your name on the web?  (1 paragraph answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your answer to Question 10\n",
    "\n",
    "(PLEASE PUT YOUR ANSWER HERE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Beyond search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 11\n",
    "\n",
    "Please read this article (https://www.scientificamerican.com/article/how-a-computer-program-helped-show-jk-rowling-write-a-cuckoos-calling/ \"How a computer program helped show JK Roweling wrote A Cuckoo's Calling\") and indicate 3 things you learned.  Also mention what idea(s) from class relate to this investigation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your answer to Question 11\n",
    "\n",
    "(PLEASE PUT YOUR ANSWER HERE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 12\n",
    "\n",
    "Please go to https://ai.googleblog.com/ and look at the posts tagged \"Natural Language Processing\".  Read one of the posts that discusses a research project (not just a post describing researchers' attendance at a conference) and summarize the research in your own words (1 paragraph).  What parts of the research do you understand?  What parts are you unfamiliar with?  Please be prepared to present/discuss these findings in class!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your answer to Question 12\n",
    "\n",
    "(PLEASE PUT YOUR ANSWER HERE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 13\n",
    "\n",
    "Please check out https://trends.google.com/trends/explore and explore what people have been searching for over time.  Write down 3 interesting observations.  For example, can you find evidence for an annual cycle in searches for terms that are related to seasonal interests?  Can you find changes in what people are interested in over the last 10 years?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your answer to Question 13\n",
    "\n",
    "goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 14\n",
    "\n",
    "Please write and answer your own question.  It can be a conceptual/writing question or a programming question.\n",
    "\n",
    "The last homework (\"Review\") will be made from these questions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your question and answer for Question 14\n",
    "\n",
    "(PLEASE PUT YOUR QUESTION AND ANSWER HERE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
